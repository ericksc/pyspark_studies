{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90dd4514",
   "metadata": {},
   "source": [
    "# The Essential PySpark Cheat Sheet for Data Engineers\n",
    "This notebook serves as a comprehensive resource for revising essential PySpark transformations, functions, and methodologies. It is tailored to support your interview preparation and enhance your proficiency with PySpark on platforms like Databricks and other Python-based environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca739b7e",
   "metadata": {},
   "source": [
    "## Getting Started with PySpark\n",
    "Before performing any transformations, begin by creating a Spark Session. This is the entry point to all PySpark functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ff0caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PySpark Application\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b23372",
   "metadata": {},
   "source": [
    "## Reading Data\n",
    "Load data from various file formats such as CSV, Excel, Parquet, or Avro. Use a custom schema for improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c900e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"dept\", StringType())\n",
    "])\n",
    "\n",
    "data = spark.read.csv(\"filePath\", header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1551af6",
   "metadata": {},
   "source": [
    "## Creating Sample DataFrame\n",
    "Use this sample for practice and transformation examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02da971e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|salary|\n",
      "+---+------+------+\n",
      "|  1|  juan|   100|\n",
      "|  2| pedro|   200|\n",
      "|  3|andres|   300|\n",
      "|  4|carlos|   400|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'juan', 100), (2, 'pedro', 200), (3, 'andres', 300), (4, 'carlos', 400)]\n",
    "schema = ['id', 'name', 'salary']\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7c7dce",
   "metadata": {},
   "source": [
    "## Window Functions\n",
    "Example: Cumulative Sum and Average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32dc0d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+--------------+\n",
      "| id|  name|salary|cumulative_sum|\n",
      "+---+------+------+--------------+\n",
      "|  1|  juan|   100|           100|\n",
      "|  2| pedro|   200|           300|\n",
      "|  3|andres|   300|           600|\n",
      "|  4|carlos|   400|          1000|\n",
      "+---+------+------+--------------+\n",
      "\n",
      "+---+------+------+--------------+\n",
      "| id|  name|salary|cumulative_avg|\n",
      "+---+------+------+--------------+\n",
      "|  1|  juan|   100|         100.0|\n",
      "|  2| pedro|   200|         150.0|\n",
      "|  3|andres|   300|         200.0|\n",
      "|  4|carlos|   400|         250.0|\n",
      "+---+------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(\"id\")\n",
    "df.withColumn(\"cumulative_sum\", sum(\"salary\").over(window_spec)).show()\n",
    "df.withColumn(\"cumulative_avg\", avg(\"salary\").over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c9dc1d",
   "metadata": {},
   "source": [
    "## Joining DataFrames\n",
    "Use `dept_id` as a join key between employees and departments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8480cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----------+\n",
      "| id|  name|salary|department|\n",
      "+---+------+------+----------+\n",
      "|  1|  juan|   100|        HR|\n",
      "|  2| pedro|   200|     Sales|\n",
      "|  3|carlos|   300|        DA|\n",
      "|  4|andres|   400|        IT|\n",
      "+---+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_data = [(1, 'juan', 100, 1), (2, 'pedro', 200, 2), (3, 'carlos', 300, 3), (4, 'andres', 400, 4)]\n",
    "emp_schema = ['id', 'name', 'salary', 'dept_id']\n",
    "employee = spark.createDataFrame(emp_data, emp_schema)\n",
    "\n",
    "dept_data = [(1, 'HR'), (2, 'Sales'), (3, 'DA'), (4, 'IT')]\n",
    "dept_schema = ['dept_id', 'department']\n",
    "department = spark.createDataFrame(dept_data, dept_schema)\n",
    "\n",
    "joined_df = employee.join(department, \"dept_id\", \"inner\")\n",
    "joined_df.select('id', 'name', 'salary', 'department').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0215e3-e651-4f63-9fac-af61025776d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
