{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fb88797-154b-4c36-b61b-12ef4e963936",
   "metadata": {},
   "source": [
    "# Basic PySpark examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94171639-5bfb-4fc4-9733-96f9477a40eb",
   "metadata": {},
   "source": [
    "## 1. Initialize SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78a0cff2-e72d-4c50-856e-c86b1373ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc599bcc-c9f3-4deb-8b91-f1c48cd48788",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BasicExample\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b1e14f-bff2-4fa5-ae9b-1e122703e4df",
   "metadata": {},
   "source": [
    "## 2. Create a DataFrame from a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74393256-3fa6-4f74-963e-c3ebf3d29cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 28)]\n",
    "columns = [\"Name\", \"Age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a723c792-2c60-4eac-8420-25ea76379b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "|Charlie| 28|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660833dc-0255-468b-b4d9-7162c2eb31a0",
   "metadata": {},
   "source": [
    "## 3. Read a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58612d62-0d58-44e7-8062-41ec5c027ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Alice| 30|\n",
      "|    Bob| 25|\n",
      "|Charlie| 35|\n",
      "|  Diana| 22|\n",
      "|    Eve| 28|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"file.csv\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f286364e-b43c-44f1-8655-46c7625f6ee1",
   "metadata": {},
   "source": [
    "## 4. Select columns and filter rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9afadf45-57b6-41cb-90c1-c9b4d5c09c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Alice| 30|\n",
      "|Charlie| 35|\n",
      "|    Eve| 28|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Name\", \"Age\").filter(df.Age > 26).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaee0d7-173c-4c7e-95a8-ad6682db78fd",
   "metadata": {},
   "source": [
    "## 5. Add a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc5815c8-8998-4ef6-a88e-25cf5dd7f2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+\n",
      "|   Name|Age|AgePlusTen|\n",
      "+-------+---+----------+\n",
      "|  Alice| 30|        40|\n",
      "|    Bob| 25|        35|\n",
      "|Charlie| 35|        45|\n",
      "|  Diana| 22|        32|\n",
      "|    Eve| 28|        38|\n",
      "+-------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"AgePlusTen\", col(\"Age\") + 10)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7cece5-f5f0-449c-ac8a-5599d08f4396",
   "metadata": {},
   "source": [
    "## 6. Group by and aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34374dc7-c84a-4f24-a037-a3b87ecae958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|Age|count|\n",
      "+---+-----+\n",
      "| 28|    1|\n",
      "| 22|    1|\n",
      "| 35|    1|\n",
      "| 25|    1|\n",
      "| 30|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Age\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33b72d4-af22-486d-a8f4-eead6798c0b5",
   "metadata": {},
   "source": [
    "## 7. Write DataFrame to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5b9b49d-13f9-434e-b4f4-3e6e526adff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"my_parquet_file.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bad4e72-95cc-4b13-a209-7e78b64ea3e2",
   "metadata": {},
   "source": [
    "## 8. Stop the SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afc72602-9b3a-4890-b2f0-d54a78eccb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf57c46-3a19-474b-8ff7-be3839c8f010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
