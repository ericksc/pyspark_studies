{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c035225",
   "metadata": {},
   "source": [
    "# Writing Parquet Files with PySpark\n",
    "This notebook demonstrates how to create a DataFrame and write it as Parquet files using PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2f4a940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Write Parquet Example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53abd26e",
   "metadata": {},
   "source": [
    "## Create a Sample DataFrame\n",
    "We create a simple DataFrame with sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41e79fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 25|\n",
      "|  2|    Bob| 30|\n",
      "|  3|Charlie| 28|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Alice\", 25),\n",
    "    (2, \"Bob\", 30),\n",
    "    (3, \"Charlie\", 28)\n",
    "]\n",
    "columns = [\"id\", \"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f106c338",
   "metadata": {},
   "source": [
    "## Write DataFrame to Parquet\n",
    "This will write the DataFrame to disk in Parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b7e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"./parquet_output/parquet_data\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a933cc05",
   "metadata": {},
   "source": [
    "## Optional: Partition and Compress Parquet Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ac7a12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write \\\n",
    "  .partitionBy(\"age\") \\\n",
    "  .option(\"compression\", \"snappy\") \\\n",
    "  .parquet(\"./parquet_output/parquet_partitioned\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfa5410",
   "metadata": {},
   "source": [
    "## Read Parquet Back into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7d84de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  3|Charlie| 28|\n",
      "|  1|  Alice| 25|\n",
      "|  2|    Bob| 30|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_loaded = spark.read.parquet(\"./parquet_output/parquet_partitioned\")\n",
    "df_loaded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1eb4b8-002d-439a-94d0-37f1ac9c2e71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
