{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "779c4f09-f025-4fef-a9a7-8683ed0572be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9e1e2d4-aceb-4376-8811-3fa5b578f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadCSVExample\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d48d5b07-6a4e-4808-9e03-7ac511c06f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    # Read the CSV file located at the specified path, treating the first row as header\n",
    "    spark.read.csv(\"./sample.csv\", header=True)\n",
    "\n",
    "    # Add a new column called 'new_column'\n",
    "    # If 'old_column' > 10, assign 10; otherwise assign 0\n",
    "    .withColumn(\n",
    "        \"new_column\", F.when(F.col(\"old_column\") > 10, 10).otherwise(0)\n",
    "    )\n",
    "\n",
    "    # Filter rows where 'old_column' is greater than 8\n",
    "    .where(\"old_column > 8\")\n",
    "\n",
    "    # Group the data by the values in 'new_column'\n",
    "    .groupby(\"new_column\")\n",
    "\n",
    "    # Count how many rows exist per 'new_column' group\n",
    "    .count()\n",
    "\n",
    "    # Write the resulting grouped data to a CSV file\n",
    "    # Overwrite the file if it already exists\n",
    "    .write.csv(\"updated_frequencies.csv\", mode=\"overwrite\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c2c864-bf2e-4cc1-b2a7-e59256722331",
   "metadata": {},
   "source": [
    "Why PySpark Output is a Folder with part-* Files\n",
    "- Distributed System:\n",
    "PySpark (Spark) is designed to run on distributed clusters. When your job runs, each executor processes a partition of the data. Each partition's result is written independently into a file.\n",
    "\n",
    "- Parallelism:\n",
    "The files like part-00000.csv, part-00001.csv, etc., correspond to individual partitions of the DataFrame, written by different workers in parallel. These are saved inside the specified output directory.\n",
    "\n",
    "- Scalability:\n",
    "Writing multiple files is efficient and scalable for big data workflows. It avoids single-node bottlenecks when writing huge files.\n",
    "\n",
    "- Metadata Files:\n",
    "You may also see files like _SUCCESS, which indicate the job completed successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9be280a-40fb-432d-9edb-b366bd77e72a",
   "metadata": {},
   "source": [
    "What to Do If You Want a Single File\n",
    "If you're working locally or with small data and want to get one CSV file, you can:\n",
    "\n",
    "Option 1: Coalesce to 1 partition\n",
    "\n",
    "```python\n",
    "df.coalesce(1).write.csv(\"output_single_file.csv\", mode=\"overwrite\", header=True)\n",
    "```\n",
    "Warning: This forces all data into one partition â€” not scalable for large datasets.\n",
    "\n",
    "Option 2: Use .toPandas().to_csv() (for small datasets)\n",
    "```python\n",
    "df.toPandas().to_csv(\"single_file.csv\", index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be58b83-d7e3-4f8f-9e00-b5d941ac31c1",
   "metadata": {},
   "source": [
    "## Example: Merge part-*.csv into One File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e64795cd-7438-42dd-9e5b-a8b8e883afd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged into: merged_output.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the Spark output directory\n",
    "input_folder = \"updated_frequencies.csv\"\n",
    "output_file = \"merged_output.csv\"\n",
    "\n",
    "# Get all part files (ignore _SUCCESS or hidden files)\n",
    "part_files = [f for f in os.listdir(input_folder) if f.startswith(\"part-\") and f.endswith(\".csv\")]\n",
    "\n",
    "# Read and concatenate all part files\n",
    "df_list = [pd.read_csv(os.path.join(input_folder, f)) for f in part_files]\n",
    "merged_df = pd.concat(df_list)\n",
    "\n",
    "# Save to a single CSV file\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Merged into: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11665dc-6710-450c-a753-7cd8998e20a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
